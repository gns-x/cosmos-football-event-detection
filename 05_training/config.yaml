# Phase 5: Training Pipeline Smoke Test Configuration
# Goal: Overfit on single batch to prove model can learn

# Model configuration
model_name: "Cosmos-Reason1-7B"
base_model_path: "nvidia/Cosmos-Reason1-7B"
model_type: "multimodal_llm"
architecture: "Qwen2.5-VL-7B-Instruct"

# LoRA configuration
lora_rank: 16
lora_alpha: 32
lora_dropout: 0.1
target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]

# Phase 5 Smoke Test Configuration
learning_rate: 1e-4
batch_size: 1                    # Single batch for overfitting
gradient_accumulation_steps: 1   # No accumulation needed
epochs: 50                        # High epochs for overfitting
max_steps: 1000
warmup_steps: 10                  # Minimal warmup

# Data configuration
train_data_path: "../04_dataset/train.jsonl"
validation_data_path: "../04_dataset/validation.jsonl"  # Will be copied from train
max_length: 2048

# Cosmos-specific video settings
video_fps: 4
max_video_frames: 10
max_image_frames: 10
input_context_length: 128000

# Output configuration
output_dir: "./checkpoints"
save_steps: 10                    # Save frequently for smoke test
eval_steps: 5                     # Evaluate frequently
logging_steps: 1                  # Log every step

# Hardware configuration
device_map: "auto"
torch_dtype: "bfloat16"
fp16: false
bf16: true

# Monitoring and logging
use_wandb: false                  # Disable for smoke test
wandb_project: "cosmos-football"
tensorboard_log_dir: "./logs"

# Other settings
seed: 42
dataloader_num_workers: 0         # Single process for smoke test
remove_unused_columns: false
