# Configuration file for Cosmos fine-tuning
# Hyperparameters and training settings

# Model configuration
model_name: "cosmos-reason-1-7b"
base_model_path: "nvidia/cosmos-reason-1-7b"

# LoRA configuration
lora_rank: 16
lora_alpha: 32
lora_dropout: 0.1
target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]

# Training configuration
learning_rate: 1e-4
batch_size: 1
gradient_accumulation_steps: 4
epochs: 3
max_steps: 1000
warmup_steps: 100

# Data configuration
train_data_path: "../04_dataset/train.jsonl"
validation_data_path: "../04_dataset/validation.jsonl"
max_length: 2048

# Output configuration
output_dir: "./checkpoints"
save_steps: 500
eval_steps: 250
logging_steps: 50

# Hardware configuration
device_map: "auto"
torch_dtype: "bfloat16"
fp16: false
bf16: true

# Other settings
seed: 42
dataloader_num_workers: 4
remove_unused_columns: false
